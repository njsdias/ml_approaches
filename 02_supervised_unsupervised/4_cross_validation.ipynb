{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "It is the process to split dataset into few parts to train and validation.We train the model on\n",
    "some of these parts and test on the remaining parts.\n",
    "\n",
    "Choosing the right cross-validation\n",
    "depends on the dataset you are dealing with, and one’s choice of cross-validation\n",
    "on one dataset may or may not apply to other datasets.\n",
    "\n",
    "Types of cross-validation techniques which are the most popular and widely used.\n",
    "These include:\n",
    "- k-fold cross-validation\n",
    "- stratified k-fold cross-validation\n",
    "- hold-out based validation\n",
    "- leave-one-out cross-validation\n",
    "- group k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide the data into k different sets which are exclusive of each other. This\n",
    "is known as **k-fold cross-validation.**\n",
    "\n",
    "We can split any data into k-equal parts using KFold from scikit-learn. Each sample\n",
    "is assigned a value from 0 to k-1 when using k-fold cross validation.\n",
    "\n",
    "Dataset -> randomize rows -> Dataset Randomized -> Select K equal parts\n",
    "\n",
    "We can use this process with almost all kinds of datasets. For example, when you\n",
    "have images, you can create a CSV with image id, image location and image label\n",
    "and use the process above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Run the code below\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #Load dataset\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    \n",
    "    # create new collumn called kfold and fill it with -1\n",
    "    df['kfold'] = -1\n",
    "    \n",
    "    # randomise dataset\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # initiate kfold class\n",
    "    kf = model_selection.KFold(n_splits=5)\n",
    "    \n",
    "    # fill the new kfold column\n",
    "    for fold, (trn_, val_) in enumerate(kf.split(X=df)):\n",
    "        df.loc[val_, 'kfold'] = fold\n",
    "    \n",
    "    # save the new csv with kfold column\n",
    "    df.to_csv(\"train_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified k-fold**. If you have a skewed dataset for binary classification with 90% positive samples and only 10% negative samples\n",
    "\n",
    "Stratified k-fold cross-validation keeps the ratio of labels in each fold constant. So,\n",
    "in each fold, you will have the same 90% positive and 10% negative samples.\n",
    "\n",
    "So, in each fold, you will have the same 90% positive and 10% negative samples. Thus, whatever metric you choose to evaluate, it will give similar results across all folds.\n",
    "\n",
    "**We assume that** our CSV dataset has a column called “target” and it is a classification problem!\n",
    "\n",
    "The code is the same as the k-fold except for `model_selection`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Run the code below\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #Load dataset\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    \n",
    "    # create new collumn called kfold and fill it with -1\n",
    "    df['kfold'] = -1\n",
    "    \n",
    "    # randomize dataset\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # initiate kfold class\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # fill the new kfold column\n",
    "    for fold, (trn_, val_) in enumerate(kf.split(X=df)):\n",
    "        df.loc[val_, 'kfold'] = fold\n",
    "    \n",
    "    # save the new csv with kfold column\n",
    "    df.to_csv(\"train_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For large amount data** we can opt for a **hold-out based validation**. \n",
    "\n",
    "This method is very frequent for **time-series data**.  let’s say our job is to predict the sales from\n",
    "time step 31 to 40. We can then keep 21 to 30 as hold-out and train our model from\n",
    "step 0 to step 20. You should note that when you are predicting from 31 to 40, you\n",
    "should include the data from 21 to 30 in your model; otherwise, performance will\n",
    "be sub-par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Problems\n",
    "\n",
    "Now we can move to regression. The good thing about regression problems is that\n",
    "we can use all the cross-validation techniques mentioned above for regression\n",
    "problems except for stratified k-fold. That is **we cannot use stratified k-fold directly**,\n",
    "but there are ways to change the problem a bit so that we can use stratified k-fold\n",
    "for regression problems. Mostly, simple k-fold cross-validation works for any\n",
    "regression problem. However, **if you see that the distribution of targets is not\n",
    "consistent, you can use stratified k-fold**.\n",
    "\n",
    "To use stratified k-fold for a regression problem, **we have first to divide the target\n",
    "into bins**, and then we can use stratified k-fold in the same way as for classification\n",
    "problems. There are several choices for selecting the appropriate number of bins. If\n",
    "you have a **lot of samples**( > 10k, > 100k), then you don’t need to care about the\n",
    "number of bins. Just **divide the data into 10 or 20 bins.** If you do **not have a lot of\n",
    "samples**, you can use a simple rule like **Sturge’s Rule to calculate the appropriate\n",
    "number of bins.**\n",
    "\n",
    "Let’s make a sample regression dataset and try to apply stratified k-fold as shown\n",
    "in the following python snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(data):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    data[\"kfold\"] = -1\n",
    "    \n",
    "    # the next step is to randomize the rows of the data\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # calculate the number of bins by Sturge's rule\n",
    "    # I take the floor of the value, you can also\n",
    "    # just round it\n",
    "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
    "    \n",
    "    # bin targets\n",
    "    data.loc[:, \"bins\"] = pd.cut(data[\"target\"], bins=num_bins, labels=False)\n",
    "    \n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # fill the new kfold column\n",
    "    # note that, instead of targets, we use bins!\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    \n",
    "    # drop the bins column\n",
    "    data = data.drop(\"bins\", axis=1)\n",
    "    \n",
    "    # return dataframe with folds\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuno/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # we create a sample dataset with 15000 samples\n",
    "    # and 100 features and 1 target\n",
    "    X, y = datasets.make_regression(n_samples=15000, n_features=100, n_targets=1)\n",
    "    \n",
    "    # create a dataframe out of our numpy arrays\n",
    "    df = pd.DataFrame(X,columns=[f\"f_{i}\" for i in range(X.shape[1])])\n",
    "    df.loc[:, \"target\"] = y\n",
    "\n",
    "    # create folds\n",
    "    df = create_folds(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation is the first and most essential step** when it comes to building\n",
    "machine learning models. \n",
    "\n",
    "If you want to do feature engineering, **split your data first**.\n",
    "If you're going to build models, split your data first. \n",
    "\n",
    "If you have a good cross-validation scheme in which validation data is representative of training and real-\n",
    "world data, you will be able **to build a good machine learning model** which is highly generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time-Series\n",
    "\n",
    "import pandas as pd\n",
    "# create a series of datetime with a frequency of 10 hours\n",
    "s = pd.date_range('2020-01-06', '2020-01-10', freq='10H').to_series()\n",
    "# create some features based on datetime\n",
    "features = {\n",
    "    \"dayofweek\": s.dt.dayofweek.values,\n",
    "    \"dayofyear\": s.dt.dayofyear.values,\n",
    "    \"hour\": s.dt.hour.values,\n",
    "    \"is_leap_year\": s.dt.is_leap_year.values,\n",
    "    \"quarter\": s.dt.quarter.values,\n",
    "    \"weekofyear\": s.dt.weekofyear.values\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggreate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    # create a bunch of features using the date column\n",
    "    df.loc[:, 'year'] = df['date'].dt.year\n",
    "    df.loc[:, 'weekofyear'] = df['date'].dt.weekofyear\n",
    "    df.loc[:, 'month'] = df['date'].dt.month\n",
    "    df.loc[:, 'dayofweek'] = df['date'].dt.dayofweek\n",
    "    df.loc[:, 'weekend'] = (df['date'].dt.weekday >=5).astype(int)\n",
    "    \n",
    "    # create an aggregate dictionary\n",
    "    aggs = {}\n",
    "    # for aggregation by month, we calculate the\n",
    "    # number of unique month values and also the mean\n",
    "    aggs['month'] = ['nunique', 'mean']\n",
    "    aggs['weekofyear'] = ['nunique', 'mean']\n",
    "    \n",
    "    # we aggregate by num1 and calculate sum, max, min\n",
    "    # and mean values of this column\n",
    "    aggs['num1'] = ['sum','max','min','mean']\n",
    "    \n",
    "    # for customer_id, we calculate the total count\n",
    "    aggs['customer_id'] = ['size']\n",
    "    # again for customer_id, we calculate the total unique\n",
    "    \n",
    "    aggs['customer_id'] = ['nunique']\n",
    "    \n",
    "    # we group by customer_id and calculate the aggregates\n",
    "    agg_df = df.groupby('customer_id').agg(aggs)\n",
    "    agg_df = agg_df.reset_index()\n",
    "\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, for example, when dealing with time-series problems, you might have\n",
    "features which are not individual values but a list of values. In these cases, you can create a bunch of statistical features such as:\n",
    "- Mean\n",
    "- Max\n",
    "- Min\n",
    "- Unique\n",
    "- Skew\n",
    "- Kurtosis\n",
    "- Kstat\n",
    "- Percentile\n",
    "- Quantile\n",
    "- Peak to peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_dict = {}\n",
    "\n",
    "# calculate mean\n",
    "feature_dict['mean'] = np.mean(x)\n",
    "\n",
    "# calculate max\n",
    "feature_dict['max'] = np.max(x)\n",
    "\n",
    "# calculate min\n",
    "feature_dict['min'] = np.min(x)\n",
    "\n",
    "# calculate standard deviation\n",
    "feature_dict['std'] = np.std(x)\n",
    "\n",
    "# calculate variance\n",
    "feature_dict['var'] = np.var(x)\n",
    "\n",
    "# peak-to-peak\n",
    "feature_dict['ptp'] = np.ptp(x)\n",
    "\n",
    "# percentile features\n",
    "feature_dict['percentile_10'] = np.percentile(x, 10)\n",
    "feature_dict['percentile_60'] = np.percentile(x, 60)\n",
    "feature_dict['percentile_90'] = np.percentile(x, 90)\n",
    "\n",
    "# quantile features\n",
    "feature_dict['quantile_5'] = np.percentile(x, 5)\n",
    "feature_dict['quantile_95'] = np.percentile(x, 95)\n",
    "feature_dict['quantile_99'] = np.percentile(x, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series data (list of values) can be converted to a lot of features using **tsfresh**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.feature_extraction import feature_calculators as fc\n",
    "# tsfresh based features\n",
    "feature_dict['abs_energy'] = fc.abs_energy(x)\n",
    "feature_dict['count_above_mean'] = fc.count_above_mean(x)\n",
    "feature_dict['count_below_mean'] = fc.count_below_mean(x)\n",
    "feature_dict['mean_abs_change'] = fc.mean_abs_change(x)\n",
    "feature_dict['mean_change'] = fc.mean_change(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple\n",
    "way to generate many features is just to **create a bunch of polynomial features**. For\n",
    "example, a second-degree polynomial feature from two features “a” and “b” would\n",
    "include: “a”, “b”, “ab”, “a²” and “b²”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate a random dataframe with\n",
    "# 2 columns and 100 rows\n",
    "\n",
    "df = pd.DataFrame(\n",
    "np.random.rand(100, 2),\n",
    "columns=[f\"f_{i}\" for i in range(1, 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two-degree polynomial features using **PolynomialFeatures** from\n",
    "scikit-learn.\n",
    "\n",
    "If you have a lot of samples in the dataset, it is going to take a while\n",
    "creating these kinds of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# initialize polynomial features class object\n",
    "# for two-degree polynomial features\n",
    "pf = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "\n",
    "# fit to the features\n",
    "pf.fit(df)\n",
    "\n",
    "# create polynomial features\n",
    "poly_feats = pf.transform(df)\n",
    "\n",
    "# create a dataframe with all the features\n",
    "num_feats = poly_feats.shape[1]\n",
    "df_transformed = pd.DataFrame(poly_feats,\n",
    "                              columns=[f\"f_{i}\" for i in range(1, num_feats + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binning** technique can generate features that cosists in divide data into N parts. \n",
    "\n",
    "Binning also enables you to treat\n",
    "numerical features as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bins of the numerical columns\n",
    "# 10 bins\n",
    "df[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)\n",
    "# 100 bins\n",
    "df[\"f_bin_100\"] = pd.cut(df[\"f_1\"], bins=100, labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log Transformation**: For example to reduce the variance of the feature.\n",
    "\n",
    "Log and exponential transformation can be used to optimize the model for the metric RMSLE. In that case,\n",
    "we can train on log-transformed targets and convert back to original using\n",
    "exponential on the prediction. That would help optimize the model for the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.f_3.apply(lambda x: np.log(1 + x)).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values in numerical data\n",
    "We can fill missing values with:\n",
    "- a values that never happears in the data: for example: 0 or 9999\n",
    "- mean of the numerical value\n",
    "- median of the numerical feature\n",
    "- mode of the numerical feature which is the value that appears most often\n",
    "\n",
    "A not usually way of filling of missing values would be to use a **k-nearest neighbour\n",
    "method.** After finding the K-NN take the mean of all nearest neighbours and fill up the missing value.\n",
    "You can use the KNN imputer implementation for filling missing values like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import impute\n",
    "\n",
    "# create a random numpy array with 10 samples\n",
    "# and 6 features and values ranging from 1 to 15\n",
    "X = np.random.randint(1, 15, (10, 6))\n",
    "\n",
    "# convert the array to float\n",
    "X = X.astype(float)\n",
    "\n",
    "# randomly assign 10 elements to NaN (missing)\n",
    "X.ravel()[np.random.choice(X.size, 10, replace=False)] = np.nan\n",
    "\n",
    "# use 3 nearest neighbours to fill na values\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=2)\n",
    "knn_imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of imputing missing values in a column would be to **train a regression\n",
    "model** that tries to predict missing values** in a column based on other columns. So,\n",
    "you start with one column that has a missing value and treat this column as the\n",
    "target column for regression model without the missing values. Using all the other\n",
    "columns, you now train a model on samples for which there is no missing value in\n",
    "the concerned column and then try to predict target (the same column) for the\n",
    "samples that were removed earlier.\n",
    "\n",
    "**NOTES:**\n",
    "- Always remember that imputing values for tree-based models is unnecessary as they\n",
    "can handle it themselves.\n",
    "- And always remember to scale or normalize your features if you are using linear models like logistic regression or a model like SVM. Tree-based models will always work fine without any normalization of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever encounter missing values in categorical features, treat is as a new category! As simple as this is, it\n",
    "**(almost) always works**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification\n",
    "\n",
    "This is for problems with more than two classes (binary classrificatoin: 0,1).\n",
    "\n",
    "Most of the metrics that we discussed for binary classification can be converted to a multi-class\n",
    "version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate True Positives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: number of positives\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true,y_pred):\n",
    "        if yt == 1 and yp ==1:\n",
    "            tp +=1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate True Negatives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :retutn: number of negatives\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true,y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn +=1\n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate False Positives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :retutn: number of fasle positives\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true,y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp +=1\n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate False Negative\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :retutn: number of false negative\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true,y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn +=1\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: precision score\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate recall\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: recall score\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    recall = tp / (tp + fn)\n",
    "                   \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take **precision and recall**. We can calculate\n",
    "precision and recall for each class in a multi-class classification problem.\n",
    "\n",
    "Let’s assume we are interested in **precision** first. We know that precision\n",
    "depends on true positives and false positives.\n",
    "- **Macro averaged precision:** calculate precision for all classes individually and then average them\n",
    "- **Micro averaged precision:** calculate class wise true positive and false positive and then use that to calculate overall precision\n",
    "- **Weighted precision:** same as macro but in this case, it is weighted average depending on the number of items in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate macro averaged precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_proba: list of predicted values\n",
    "    :return: macro precision score\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in range(num_classes):\n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "\n",
    "        # calculate true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "\n",
    "        # calculate precision for current class\n",
    "        temp_precision = tp / (tp + fp)\n",
    "\n",
    "        # keep adding precision for all classes\n",
    "        precision += temp_precision\n",
    "\n",
    "        # calculate and return average precision over all classes\n",
    "        precision /= num_classes\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate micro averaged precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_proba: list of predicted values\n",
    "    :return: micro precision score\n",
    "    \"\"\"\n",
    "\n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "\n",
    "    # loop over all classes\n",
    "    for class_ in range(num_classes):\n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "\n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "\n",
    "        # calculate false positive for current class\n",
    "        # and update overall tp\n",
    "        fp += false_positive(temp_true, temp_pred)\n",
    "\n",
    "        # calculate and return overall precision\n",
    "        precision = tp / (tp + fp)\n",
    "    \n",
    "    return precision\n",
    "                          \n",
    "\n",
    "def weighted_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate weighted averaged precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_proba: list of predicted values\n",
    "    :return: weighted precision score\n",
    "    \"\"\"\n",
    "\n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # create class:sample count dictionary\n",
    "    # it looks something like this:\n",
    "    # {0: 20, 1:15, 2:21}\n",
    "    class_counts = Counter(y_true)\n",
    "\n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "\n",
    "    # loop over all classes\n",
    "    for class_ in range(num_classes):\n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "\n",
    "        # calculate tp and fp for class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "\n",
    "        # calculate precision of class\n",
    "        temp_precision = tp / (tp + fp)\n",
    "\n",
    "        # multiply precision with count of samples in class\n",
    "        weighted_precision = class_counts[class_] * temp_precision\n",
    "\n",
    "        # add to overall precision\n",
    "        precision += weighted_precision\n",
    "\n",
    "    # calculate overall precision by dividing by\n",
    "    # total number of samples\n",
    "    overall_precision = precision / len(y_true)\n",
    "    \n",
    "    return overall_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our function macro precision:  0.13888888888888887\n",
      "sklearn metrics macro precision:  0.3611111111111111\n",
      "\n",
      "\n",
      "our function micro precision:  0.4444444444444444\n",
      "sklearn metrics micro precision:  0.4444444444444444\n",
      "\n",
      "\n",
      "our function weighted precision:  0.39814814814814814\n",
      "sklearn metrics weighted precision:  0.39814814814814814\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n",
    "\n",
    "print('our function macro precision: ', macro_precision(y_true, y_pred))\n",
    "print('sklearn metrics macro precision: ',metrics.precision_score(y_true, y_pred, average=\"macro\"))\n",
    "print('\\n')\n",
    "print('our function micro precision: ',micro_precision(y_true, y_pred))\n",
    "print('sklearn metrics micro precision: ',metrics.precision_score(y_true, y_pred, average=\"micro\"))\n",
    "print('\\n')\n",
    "print('our function weighted precision: ',weighted_precision(y_true, y_pred))\n",
    "print('sklearn metrics weighted precision: ',metrics.precision_score(y_true, y_pred, average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can implement the **recall metric for multi-class**. \n",
    "\n",
    "Precision and recall depend on\n",
    "- true positive, \n",
    "- false positive \n",
    "- false negative \n",
    "\n",
    "While F1 depends on\n",
    "- precision and recall.\n",
    "\n",
    "Implementation for recall is left as an exercise for the reader and one version of F1\n",
    "for multi-class, i.e., weighted average is implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def weighted_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate weighted f1 score\n",
    "    :param y_true: list of true values\n",
    "    :param y_proba: list of predicted values\n",
    "    :return: weighted f1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # create class:sample count dictionary\n",
    "    # it looks something like this:\n",
    "    # {0: 20, 1:15, 2:21}\n",
    "    class_counts = Counter(y_true)\n",
    "\n",
    "    # initialize f1 to 0\n",
    "    f1 = 0\n",
    "\n",
    "    # loop over all classes\n",
    "    for class_ in range(num_classes):\n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "\n",
    "        # calculate precision and recall for class\n",
    "        p = precision(temp_true, temp_pred)\n",
    "        r = recall(temp_true, temp_pred)\n",
    "        # calculate\n",
    "        if p + r !=0:\n",
    "            temp_f1 = 2 * p * r / (p + r)\n",
    "        else:\n",
    "            temp_f1 = 0\n",
    "\n",
    "        # multiply f1 with count of samples in class\n",
    "        weighted_f1 = class_counts[class_] * temp_f1\n",
    "\n",
    "        # add to f1 precision\n",
    "        f1 += weighted_f1\n",
    "\n",
    "    # calculate overall F1 by dividing by\n",
    "    # total number of samples\n",
    "    overall_f1 = f1 / len(y_true)\n",
    "        \n",
    "    return overall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our function weighted f1:  0.41269841269841273\n",
      "sklearn metrics weighted f1:  0.41269841269841273\n"
     ]
    }
   ],
   "source": [
    "y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n",
    "    \n",
    "print('our function weighted f1: ',weighted_f1(y_true, y_pred))\n",
    "print('sklearn metrics weighted f1: ',metrics.f1_score(y_true, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have precision, recall and F1 implemented for multi-class problems. You\n",
    "can similarly convert AUC and log loss to multi-class formats too. This format of\n",
    "conversion is known as one-vs-all. I’m not going to implement them here as the\n",
    "implementation is quite similar to what we have already discussed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

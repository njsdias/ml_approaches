{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "This suject is related with the extraction of the best performance of the models.\n",
    "\n",
    "In this sense, we can say, improvements can be done in every model. \n",
    "In this context, a good model can be improved do fine-tunning of them hyperparameters control the\n",
    "training/fitting process of the model. Thus the task is find the best value combination that comes up with the best model result.\n",
    "\n",
    "This field is under research, for instance:\n",
    "- On Linear Identifiability of Learned Representations\n",
    "- Linear Mode Connectivity and the Lottery Ticket Hypothesis\n",
    "- Deep Ensembles: A Loss Landscape Perspective\n",
    "\n",
    "In general we have two techniques:\n",
    "- **grid search:** high time consumption\n",
    "- **random search:** less time consumption\n",
    "\n",
    "With **grid search** we provide the possible values to\n",
    "that each hyperparameter can take. The technique will run the model for all value combinations. However, this technique easly take a lot of time for computation all possibilities if the dataset is too large. For these reasons, **grid search is not very popular.**\n",
    "\n",
    "Here we are going use this dataset:\n",
    "- https://www.kaggle.com/iabhishekofficial/mobile-price-classification\n",
    "\n",
    "Video explain from author:\n",
    "https://www.youtube.com/watch?v=5nYqK-HaoKY&list=PLjMBCjnfVRHQZGxbCcpd41Fm4nfBPVnCa&index=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/iabhishekofficial/mobile-price-classification\n",
    "# rf_grid_search.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # read the training data\n",
    "    df = pd.read_csv(\"input/mobile_train.csv\")\n",
    "    \n",
    "    # features are all columns without price_range\n",
    "    # note that there is no id column in this dataset\n",
    "    # here we have training features\n",
    "    x = df.drop(\"price_range\", axis=1).values\n",
    "    \n",
    "    # and the targets\n",
    "    y = df.price_range.values\n",
    "    \n",
    "    # define the model here\n",
    "    # random forest with n_jobs=-1\n",
    "    # n_jobs=-1 => use all cores\n",
    "    classifier = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "    # define a grid of parameters\n",
    "    # this can be a dictionary or a list of\n",
    "    # dictionaries\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [100, 200, 250, 300, 400, 500],\n",
    "        \"max_depth\": [1, 2, 5, 7, 11, 15],\n",
    "        \"criterion\": [\"gini\", \"entropy\"]}\n",
    "    \n",
    "    # initialize grid search\n",
    "    # estimator is the model that we have defined\n",
    "    # param_grid is the grid of parameters\n",
    "    # we use accuracy as our metric. you can define your own\n",
    "    # higher value of verbose implies a lot of details are printed\n",
    "    # cv=5 means that we are using 5 fold cv (not stratified)\n",
    "    model = model_selection.GridSearchCV(estimator=classifier,\n",
    "                                         param_grid=param_grid,\n",
    "                                         scoring=\"accuracy\",\n",
    "                                         verbose=10,\n",
    "                                         n_jobs=1,\n",
    "                                         cv=5)\n",
    "    # fit the model and extract best score\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    print(f\"Best score: {model.best_score_}\")\n",
    "    print(\"Best parameters set:\")\n",
    "    \n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(f\"\\t{param_name}: {best_parameters[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In **random search**, a set of parameter combination was chossen randomly and\n",
    "calculate the cross-validation score is calculated. Here, we choose how many times we want to evaluate our models. \n",
    "This define the time consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_random_search.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # read the training data\n",
    "    df = pd.read_csv(\"input/mobile_train.csv\")\n",
    "    \n",
    "    \n",
    "    # features are all columns without price_range\n",
    "    # note that there is no id column in this dataset\n",
    "    # here we have training features\n",
    "    X = df.drop(\"price_range\", axis=1).values\n",
    "    \n",
    "    # and the targets\n",
    "    y = df.price_range.values\n",
    "    \n",
    "    # define the model here\n",
    "    # random forest with n_jobs=-1\n",
    "    # n_jobs=-1 => use all cores\n",
    "    classifier = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "    # define a grid of parameters\n",
    "    # this can be a dictionary or a list of\n",
    "    # dictionaries\n",
    "    param_grid = {\"n_estimators\": np.arange(100, 1500, 100),\n",
    "                  \"max_depth\": np.arange(1, 31),\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]\n",
    "    }\n",
    "    \n",
    "    # initialize random search\n",
    "    # estimator is the model that we have defined\n",
    "    # param_distributions is the grid/distribution of parameters\n",
    "    # we use accuracy as our metric. you can define your own\n",
    "    # higher value of verbose implies a lot of details are printed\n",
    "    # cv=5 means that we are using 5 fold cv (not stratified)\n",
    "    # n_iter is the number of iterations we want\n",
    "    # if param_distributions has all the values as list,\n",
    "    # random search will be done by sampling without replacement\n",
    "    # if any of the parameters come from a distribution,\n",
    "    # random search uses sampling with replacement\n",
    "    model = model_selection.RandomizedSearchCV(estimator=classifier,\n",
    "                                               param_distributions=param_grid,\n",
    "                                               n_iter=20,\n",
    "                                               scoring=\"accuracy\",\n",
    "                                               verbose=10,\n",
    "                                               n_jobs=1,\n",
    "                                               cv=5)\n",
    "    \n",
    "    # fit the model and extract best score\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    print(f\"Best score: {model.best_score_}\")\n",
    "    print(\"Best parameters set:\")\n",
    "    \n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(f\"\\t{param_name}: {best_parameters[param_name]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result obtained gives the:\n",
    "- Best score: 0.888\n",
    "- Best parameters set:\n",
    "    - criterion: entropy\n",
    "    - max_depth: 25\n",
    "    - n_estimators: 1100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you have two text columns to predict a class. We can build a strategy:\n",
    "- apply tf-idf in a semi supervised manner\n",
    "- after that, use Singular Value Decomposition with SVM\n",
    "\n",
    "For this strategy works we need:\n",
    "- select the components of SVD\n",
    "- and, to tune the parameters of SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_search.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def quadractic_weighted_kappa(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Create a wrapper for cohen's kappa\n",
    "    with quadratic weights\n",
    "    \"\"\"\n",
    "    return metrics.cohen_kappa_score(y_true,y_pred,\n",
    "                                     weights=\"quadratic\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the training file\n",
    "    train = pd.read_csv('../input/train.csv')\n",
    "\n",
    "    # we dont need ID columns\n",
    "    idx = test.id.values.astype(int)\n",
    "    train = train.drop('id', axis=1)\n",
    "    test = test.drop('id', axis=1)\n",
    "    \n",
    "    # do some lambda magic on text columns\n",
    "    train_data = list(train.apply(lambda x:'%s %s' % (x['text1'], x['text2']),axis=1))\n",
    "    test_data = list(test.apply(lambda x:'%s %s' % (x['text1'], x['text2']),axis=1))\n",
    "    \n",
    "    # tfidf vectorizer\n",
    "    tfv = TfidfVectorizer(min_df=3,\n",
    "                          max_features=None,\n",
    "                          strip_accents='unicode',\n",
    "                          analyzer='word',\n",
    "                          token_pattern=r'\\w{1,}',\n",
    "                          ngram_range=(1, 3),\n",
    "                          use_idf=1,\n",
    "                          smooth_idf=1,\n",
    "                          sublinear_tf=1,\n",
    "                          stop_words='english')\n",
    "    \n",
    "    # Fit TFIDF\n",
    "    tfv.fit(traindata)\n",
    "    \n",
    "    X = tfv.transform(traindata)\n",
    "    X_test = tfv.transform(testdata)\n",
    "\n",
    "    # Initialize SVD\n",
    "    svd = TruncatedSVD()\n",
    "\n",
    "    # Initialize the standard scaler\n",
    "    scl = StandardScaler()\n",
    "    \n",
    "    # We will use SVM here..\n",
    "    svm_model = SVC()\n",
    "    \n",
    "    # Create the pipeline\n",
    "    clf = pipeline.Pipeline([('svd', svd),\n",
    "                             ('scl', scl),\n",
    "                             ('svm', svm_model)])\n",
    "    \n",
    "    # Create a parameter grid to search for\n",
    "    # best parameters for everything in the pipeline\n",
    "    param_grid = {'svd__n_components' : [200, 300],\n",
    "                  'svm__C': [10, 12]}\n",
    "\n",
    "    # Kappa Scorer\n",
    "    kappa_scorer = metrics.make_scorer(quadratic_weighted_kappa,\n",
    "                                       greater_is_better=True)\n",
    "    \n",
    "    # Initialize Grid Search Model\n",
    "    model = model_selection.GridSearchCV(estimator=clf,\n",
    "                                         param_grid=param_grid,\n",
    "                                         scoring=kappa_scorer,\n",
    "                                         verbose=10,\n",
    "                                         n_jobs=-1,\n",
    "                                         refit=True,\n",
    "                                         cv=5)\n",
    "    \n",
    "    # Fit Grid Search Model\n",
    "    model.fit(X, y)\n",
    "\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "\n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        # Get best model\n",
    "        best_model = model.best_estimator_\n",
    "        # Fit model with best parameters optimized for QWK\n",
    "        best_model.fit(X, y)\n",
    "        preds = best_model.predict(...)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization using Gaussian Process\n",
    "\n",
    "Using `gp_minimize` function allow us to use Bayesian optimization with Gaussian process.\n",
    "\n",
    "**Remember:** We cannot minimize the accuracy, but we can\n",
    "minimize it when we multiply it by -1. This way, we are minimizing the negative\n",
    "of accuracy, but in fact, we are maximizing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_gp_minimize.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt import space\n",
    "\n",
    "def optimize(params, param_names, x, y):\n",
    "    \"\"\"\n",
    "    The main optimization function.\n",
    "    This function takes all the arguments from the search space\n",
    "    and training features and targets. It then initializes\n",
    "    the models by setting the chosen parameters and runs\n",
    "    cross-validation and returns a negative accuracy score\n",
    "    :param params: list of params from gp_minimize\n",
    "    :param param_names: list of param names. order is important!\n",
    "    :param x: training data\n",
    "    :param y: labels/targets\n",
    "    :return: negative accuracy after 5 folds\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert params to dictionary\n",
    "    params = dict(zip(param_names, params))\n",
    "\n",
    "    # initialize model with current parameters\n",
    "    model = ensemble.RandomForestClassifier(**params)\n",
    "\n",
    "    # initialize stratified k-fold\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # initialize accuracy list\n",
    "    accuracies = []\n",
    "    \n",
    "    for idx in kf.split(X=x, y=y):\n",
    "        train_idx, test_idx = idx[0], idx[1]\n",
    "        \n",
    "        xtrain = x[train_idx]\n",
    "        ytrain = y[train_idx]\n",
    "\n",
    "        xtest = x[test_idx]\n",
    "        ytest = y[test_idx]\n",
    "        \n",
    "        # fit model for current fold\n",
    "        model.fit(xtrain, ytrain)\n",
    "\n",
    "        #create predictions\n",
    "        preds = model.predict(xtest)\n",
    "\n",
    "        # calculate and append accuracy\n",
    "        fold_accuracy = metrics.accuracy_score(ytest,preds)\n",
    "        accuracies.append(fold_accuracy)\n",
    "    \n",
    "    # return negative accuracy\n",
    "    return -1 * np.mean(accuracies)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # read the training data\n",
    "    df = pd.read_csv(\"../input/mobile_train.csv\")\n",
    "\n",
    "    #features are all columns without price_range\n",
    "    #note that there is no id column in this dataset\n",
    "    #here we have training features\n",
    "    X= df.drop(\"price_range\", axis=1).values\n",
    "    #and the targets\n",
    "    y= df.price_range.values\n",
    "\n",
    "    # define a parameter space\n",
    "    param_space = [\n",
    "        # max_depth is an integer between 3 and 15\n",
    "        space.Integer(3, 15, name=\"max_depth\"),\n",
    "        # n_estimators is an integer between 50 and 1500\n",
    "        space.Integer(100, 1500, name=\"n_estimators\"),\n",
    "        # criterion is a category. here we define list of categories\n",
    "        space.Categorical([\"gini\", \"entropy\"], name=\"criterion\"),\n",
    "        # you can also have Real numbered space and define a\n",
    "        # distribution you want to pick it from\n",
    "        space.Real(0.01, 1, prior=\"uniform\", name=\"max_features\")]\n",
    "    \n",
    "    param_names= [\"max_depth\",\n",
    "                  \"n_estimators\",\n",
    "                  \"criterion\",\n",
    "                  \"max_features\"]\n",
    "    \n",
    "    # by using functools partial, \n",
    "    # new function is created which has same parameters as the\n",
    "    # optimize function except for the fact that\n",
    "    # only one param, i.e. the \"params\" parameter is\n",
    "    # required. this is how gp_minimize expects the\n",
    "    # optimization function to be. you can get rid of this\n",
    "    # by reading data inside the optimize function or by\n",
    "    # defining the optimize function here.\n",
    "    optimization_function = partial(optimize,\n",
    "                                    param_names=param_names,\n",
    "                                    x=X, y=y)\n",
    "    \n",
    "    # now we call gp_minimize from scikit-optimize\n",
    "    # gp_minimize uses bayesian optimization for\n",
    "    # minimization of the optimization function.\n",
    "    # we need a space of parameters, the function itself,\n",
    "    # the number of calls/iterations we want to have\n",
    "    result = gp_minimize(optimization_function,\n",
    "                         dimensions=param_space,\n",
    "                         n_calls=15,\n",
    "                         n_random_starts=10,\n",
    "                         verbose=10)\n",
    "    \n",
    "    # create best params dict and print it\n",
    "    best_params = dict(zip(param_names,result.x))\n",
    "    \n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt\n",
    "\n",
    "The library `hyperopt` for hyperparameter optimization uses Tree-structured Parzen\n",
    "Estimator (TPE) to find the most optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_hyperopt.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "def optimize(params, x, y):\n",
    "    \"\"\"\n",
    "    The main optimization function.\n",
    "    This function takes all the arguments from the search space\n",
    "    and training features and targets. It then initializes\n",
    "    the models by setting the chosen parameters and runs\n",
    "    cross-validation and returns a negative accuracy score\n",
    "    :param params: dict of params from hyperopt\n",
    "    :param x: training data\n",
    "    :param y: labels/targets\n",
    "    :return: negative accuracy after 5 folds\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize stratified k-fold\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # initialize accuracy list\n",
    "    accuracies = []\n",
    "    \n",
    "    for idx in kf.split(X=x, y=y):\n",
    "        train_idx, test_idx = idx[0], idx[1]\n",
    "        \n",
    "        xtrain = x[train_idx]\n",
    "        ytrain = y[train_idx]\n",
    "\n",
    "        xtest = x[test_idx]\n",
    "        ytest = y[test_idx]\n",
    "        \n",
    "        # fit model for current fold\n",
    "        model.fit(xtrain, ytrain)\n",
    "\n",
    "        #create predictions\n",
    "        preds = model.predict(xtest)\n",
    "\n",
    "        # calculate and append accuracy\n",
    "        fold_accuracy = metrics.accuracy_score(ytest,preds)\n",
    "        accuracies.append(fold_accuracy)\n",
    "    \n",
    "    # return negative accuracy\n",
    "    return -1 * np.mean(accuracies)\n",
    "\n",
    "    \n",
    "if __name__ = \"__main__\":\n",
    "    # read the training data\n",
    "    df = pd.read_csv(\"../input/mobile_train.csv\")\n",
    "    \n",
    "    # features are all collumns without price_ranges\n",
    "    # note there is no id in this dataset\n",
    "    # here we have training features\n",
    "    X = df.drop(\"price_range\", axis=1).values\n",
    "    # and the targets\n",
    "    y = df.price_range.values\n",
    "    \n",
    "    # define a parameter space\n",
    "    # now we use hyperopt\n",
    "    param_space = {\n",
    "        # quniform gives round(uniform(low, high) / q) * q\n",
    "        # we want int values for depth and estimators\n",
    "        \"max_depth\": scope.int(hp.quniform(\"max_depth\", 1, 15, 1)),\n",
    "        \"n_estimators\": scope.int(hp.quniform(\"n_estimators\", 100, 1500, 1)),\n",
    "        \n",
    "        # choice chooses from a list of values\n",
    "        \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        \n",
    "        # uniform chooses a value between two values\n",
    "        \"max_features\": hp.uniform(\"max_features\", 0, 1) \n",
    "    }\n",
    "    # partial function\n",
    "    optimization_function = partial(optimize,\n",
    "                                    x=X,\n",
    "                                    y=y)\n",
    "    \n",
    "    # initialize trials to keep logging information\n",
    "    trials = Trials()\n",
    "    \n",
    "    # run hyperopt\n",
    "    hopt = fmin(fn=optimization_function,\n",
    "                space=param_space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=15,\n",
    "                trials=trials)\n",
    "    \n",
    "    print(hopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are most usually ways of tuning hyperparameters and they can be used in\n",
    "- linear regression,\n",
    "- logistic regression\n",
    "- tree-based methods\n",
    "- gradient boosting models\n",
    "    - xgboost,\n",
    "    - lightgbm,\n",
    "    - and even neural networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

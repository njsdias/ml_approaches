{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Given a text file with a description of a movie, predict if the movie is has a positive, negative or neutral evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the action that splits a sentence into a list of words.\n",
    "\n",
    "NLTK (Natual Language ToolKit) is a popular tokenization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nuno/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# your sentence\n",
    "sentence = \"hi, how are you?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare this two methods\n",
    "\n",
    "The first one only splits the senteces using the space. So, we have words mixed with pontuation.\n",
    "\n",
    "THe second one uses the NLTK whicg separeates in a propriated way the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi,', 'how', 'are', 'you?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', ',', 'how', 'are', 'you', '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "\n",
    "Counts how many times an word appears in all sentences (corpus).\n",
    "It stors the information in a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "\"hello, how are you?\",\n",
    "\"im getting bored at home. And you? What do you think?\",\n",
    "\"did you know about counts\",\n",
    "\"let's see if this works!\",\n",
    "\"YES!!!!\"\n",
    "]\n",
    "# initialize CountVectorizer\n",
    "ctv = CountVectorizer()\n",
    "# fit the vectorizer on corpus\n",
    "ctv.fit(corpus)\n",
    "corpus_transformed = ctv.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 22)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 22)\t2\n",
      "  (2, 0)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 22)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 20)\t1\n",
      "  (4, 21)\t1\n"
     ]
    }
   ],
   "source": [
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 9, 'how': 11, 'are': 2, 'you': 22, 'im': 13, 'getting': 8, 'bored': 4, 'at': 3, 'home': 10, 'and': 1, 'what': 19, 'do': 7, 'think': 17, 'did': 6, 'know': 14, 'about': 0, 'counts': 5, 'let': 15, 'see': 16, 'if': 12, 'this': 18, 'works': 20, 'yes': 21}\n"
     ]
    }
   ],
   "source": [
    "print(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "In the second sentence the word 'you' appears two times:\n",
    "    ``(2, 22) 1``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can integrate word_tokenize into CountVectorizer to **take account special characters**.\n",
    "In this case:\n",
    "- stop pontuation (.)\n",
    "- exclamation pontuation (!)\n",
    "- comma (,)\n",
    "- splits words that uses the charcater (')\n",
    "    - let'\n",
    "    - 's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n"
     ]
    }
   ],
   "source": [
    "# create a corpus of sentences\n",
    "corpus = [\"hello, how are you?\",\n",
    "          \"im getting bored at home. And you? What do you think?\",\n",
    "          \"did you know about counts\",\n",
    "          \"let's see if this works!\",\n",
    "          \"YES!!!!\"]\n",
    "\n",
    "# initialize CountVectorizer with word_tokenize from nltk\n",
    "# as the tokenizer\n",
    "ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "ctv.fit(corpus)\n",
    "\n",
    "corpus_transformed = ctv.transform(corpus)\n",
    "print(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "- TF-IDF: `imbd_tfidf.py`. It is calculate using the follow information:\n",
    "    - TF = (Number of times a term X appears in a document) / (Total numbers of terms in the document)\n",
    "    - IDF = (Total number of documents) / (Number of documents with term X in it )\n",
    "    - TF-IDF(X) = TF(X) * IDF(X)\n",
    "\n",
    "We see that instead of integer values, this time we get floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 27)\t0.2965698850220162\n",
      "  (0, 16)\t0.4428321995085722\n",
      "  (0, 14)\t0.4428321995085722\n",
      "  (0, 7)\t0.4428321995085722\n",
      "  (0, 4)\t0.35727423026525224\n",
      "  (0, 2)\t0.4428321995085722\n",
      "  (1, 27)\t0.35299699146792735\n",
      "  (1, 24)\t0.2635440111190765\n",
      "  (1, 22)\t0.2635440111190765\n",
      "  (1, 18)\t0.2635440111190765\n",
      "  (1, 15)\t0.2635440111190765\n",
      "  (1, 13)\t0.2635440111190765\n",
      "  (1, 12)\t0.2635440111190765\n",
      "  (1, 9)\t0.2635440111190765\n",
      "  (1, 8)\t0.2635440111190765\n",
      "  (1, 6)\t0.2635440111190765\n",
      "  (1, 4)\t0.42525129752567803\n",
      "  (1, 3)\t0.2635440111190765\n",
      "  (2, 27)\t0.31752680284846835\n",
      "  (2, 19)\t0.4741246485558491\n",
      "  (2, 11)\t0.4741246485558491\n",
      "  (2, 10)\t0.4741246485558491\n",
      "  (2, 5)\t0.4741246485558491\n",
      "  (3, 25)\t0.38775666010579296\n",
      "  (3, 23)\t0.38775666010579296\n",
      "  (3, 21)\t0.38775666010579296\n",
      "  (3, 20)\t0.38775666010579296\n",
      "  (3, 17)\t0.38775666010579296\n",
      "  (3, 1)\t0.38775666010579296\n",
      "  (3, 0)\t0.3128396318588854\n",
      "  (4, 26)\t0.2959842226518677\n",
      "  (4, 0)\t0.9551928286692534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = [\"hello, how are you?\",\n",
    "          \"im getting bored at home. And you? What do you think?\",\n",
    "          \"did you know about counts\",\n",
    "          \"let's see if this works!\",\n",
    "          \"YES!!!!\"]\n",
    "\n",
    "#initialize TfidfVectorizer with word_tokenize from nltk\n",
    "# as the tokenizer\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "# fit the vectorizer on corpus\n",
    "tfv.fit(corpus)\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "\n",
    "Making sets of words. The order is important.\n",
    "\n",
    "Until now we are considered one word (one-gram). With N-gram we can \n",
    "considered a sets of words that become a part of our vocabulary. \n",
    "\n",
    "We can use n-gram as a parameter in CountVectorizer and TfidfVectorize. By default \n",
    "the minimum and maximum limit are (1,1). We can change it to (1,3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', ',', 'how'), (',', 'how', 'are'), ('how', 'are', 'you'), ('are', 'you', '?')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "# let's see 3 grams\n",
    "N = 3\n",
    "# input sentence\n",
    "sentence = \"hi, how are you?\"\n",
    "# tokenized sentence\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "# generate n_grams\n",
    "n_grams = list(ngrams(tokenized_sentence, N))\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "- These techniques reduce the word to its smallest form.\n",
    "- Lemmatization is more agressive than Stemmig.\n",
    "- Stemming is more popular and widely used.\n",
    "- Types of stemmers and lemmatizers:\n",
    "    - Snowball Stemmer\n",
    "    - WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nuno/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=fishing\n",
      "stemmed_word=fish\n",
      "lemma=fishing\n",
      "\n",
      "word=fishes\n",
      "stemmed_word=fish\n",
      "lemma=fish\n",
      "\n",
      "word=fished\n",
      "stemmed_word=fish\n",
      "lemma=fished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "words = [\"fishing\", \"fishes\", \"fished\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"word={word}\")\n",
    "    print(f\"stemmed_word={stemmer.stem(word)}\")\n",
    "    print(f\"lemma={lemmatizer.lemmatize(word)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

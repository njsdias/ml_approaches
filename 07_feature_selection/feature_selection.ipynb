{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "- **Low variance feature:** this type of eature are close to being constant; Scikit-learn has an\n",
    "implementation for **VarianceThreshold** that does precisely this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "data = ...\n",
    "var_thresh = VarianceThreshold(threshold=0.1)\n",
    "transformed_data = var_thresh.fit_transform(data)\n",
    "# transformed data will have all columns with variance less\n",
    "# than 0.1 removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **High correlation**: if we find features that are correlated with each other we can discard one of them. **Pearson Correlation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# fetch a regression dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"]\n",
    "\n",
    "col_names = data[\"feature_names\"]\n",
    "\n",
    "# convert to pandas dataframe\n",
    "df = pd.DataFrame(X, columns=col_names)\n",
    "\n",
    "# introduce a highly correlated column\n",
    "df.loc[:, \"MedInc_Sqrt\"] = df.MedInc.apply(np.sqrt)\n",
    "\n",
    "# get correlation matrix (pearson)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection\n",
    "\n",
    "It consist to but a score of each feature against a given target.\n",
    "\n",
    "The most popular methods for univariate feature selection:\n",
    "- **Mutual information** \n",
    "- **ANOVA** \n",
    "- **F-test**\n",
    "- **chi square**: only for data which is non-negative in nature. Useful for NLP when we have a bag of words or tf-idf based features. \n",
    "\n",
    "There are two ways of using these in scikit- learn.\n",
    "- **SelectKBest**: It keeps the top-k scoring features\n",
    "- **SelectPercentile**: It keeps the top features which are in a percentage specified by the user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, f_classif, f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "\n",
    "class UnivariateFeatureSelection:\n",
    "    def __init__(self, n_features, problem_type, scoring):\n",
    "        \"\"\"\n",
    "        Custom univariate feature selection wrapper on\n",
    "        different univariate feature selection models from scikit-learn.\n",
    "        :param n_features: SelectPercentile if float else SelectKBest\n",
    "        :param problem_type: classification or regression\n",
    "        :param scoring: scoring function, string\n",
    "        \"\"\"\n",
    "        \n",
    "        # for a given problem type, there are only\n",
    "        # a few valid scoring methods\n",
    "        if problem_type == \"classification\":\n",
    "            valid_scoring = {\n",
    "                \"f_classif\": f_classif,\n",
    "                \"chi2\": chi2,\n",
    "                \"mutual_info_classif\": mutual_info_classif\n",
    "            }\n",
    "        else:\n",
    "            valid_scoring = {\n",
    "                \"f_regression\": f_regression,\n",
    "                \"mutual_info_regression\": mutual_info_regression    \n",
    "            }\n",
    "        \n",
    "        # raise exception if we do not have a valid scoring method\n",
    "        if scoring not in valid_scoring:\n",
    "            raise Exception(\"Invalid scoring function\")\n",
    "        \n",
    "        # if n_features is int, we use selectkbest\n",
    "        # if n_features is float, we use selectpercentile\n",
    "        # please note that it is int in both cases in sklearn\n",
    "        if isinstance(n_features, int):\n",
    "            self.selection = SelectKBest(valid_scoring[scoring],\n",
    "                                        k=n_features)\n",
    "        elif isinstance(n_features, float):\n",
    "            self.selection = SelectPercentile(valid_scoring[scoring],\n",
    "                                              percentile=int(n_features * 100))\n",
    "        else:\n",
    "            raise Exception(\"Invalid type of feature\")\n",
    "\n",
    "    # same fit function\n",
    "    def fit(self, X, y):\n",
    "        return self.selection.fit(X, y)\n",
    "\n",
    "    # same transform function\n",
    "    def transform(self, X):\n",
    "        return self.selection.transform(X)\n",
    "\n",
    "    # same fit_transform function\n",
    "    def fit_transform(self, X, y):\n",
    "        return self.selection.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufs = UnivariateFeatureSelection(n_features=0.1,\n",
    "                                 problem_type=\"regression\",\n",
    "                                 scoring =\"f_regression\"\n",
    "                                )\n",
    "ufs.fit(X,y)\n",
    "X_transformed = ufs.trasform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection using Machine Learning\n",
    "\n",
    "Univariate feature selection may not always perform\n",
    "well. Most of the time, people prefer doing feature selection using a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greddy Feature Selection\n",
    "\n",
    "- 1rst: choose a model\n",
    "- 2nd: select a loss/scoring function\n",
    "- 3rd: iteratively evaluate each feature and add it to the list of “good” features if it improves loss/score\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- the computational cost is very high.\n",
    "- take a lot of time for this kind of feature selection to finish. \n",
    "- maybe it end up overfitting the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy.py\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreddyFeatureSelection:\n",
    "    \"\"\"\n",
    "    A simple and custom class for greedy feature selection.\n",
    "    You will need to modify it quite a bit to make it suitable\n",
    "    for your dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def evaluate_score(self, X, y):\n",
    "        \"\"\"\n",
    "        This function evaluates model on data and returns\n",
    "        Area Under ROC Curve (AUC)\n",
    "        NOTE: We fit the data and calculate AUC on same data.\n",
    "        WE ARE OVERFITTING HERE.\n",
    "        But this is also a way to achieve greedy selection.\n",
    "        k-fold will take k times longer.\n",
    "        If you want to implement it in really correct way,\n",
    "        calculate OOF AUC and return mean AUC over k folds.\n",
    "        This requires only a few lines of change and has been\n",
    "        shown a few times in this book.\n",
    "        :param X: training data\n",
    "        :param y: targets\n",
    "        :return: overfitted area under the roc curve\n",
    "        \"\"\"\n",
    "        # fit the logistic regression model,\n",
    "        # and calculate AUC on same data\n",
    "        # again: BEWARE\n",
    "        # you can choose any model that suits your data\n",
    "        model = linear_model.LogisticRegression()\n",
    "        model.fit(X, y)\n",
    "        predictions = model.predict_proba(X)[:, 1]\n",
    "        auc = metrics.roc_auc_score(y, predictions)\n",
    "        return auc\n",
    "\n",
    "    def _feature_selection(self, X, y):\n",
    "        \"\"\"\n",
    "        This function does the actual greedy selection\n",
    "        :param X: data, numpy array\n",
    "        :param y: targets, numpy array\n",
    "        :return: (best_scores, best_features)\n",
    "        \"\"\"\n",
    "        # initialize good features list\n",
    "        # and best scores to keep track of both\n",
    "        good_features = []\n",
    "        best_scores = []\n",
    "        \n",
    "        # calculate the number of features\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        # infinite loop\n",
    "        while True:\n",
    "            # initialize best feature and score of this loop\n",
    "            this_feature = None\n",
    "            best_score = 0\n",
    "\n",
    "            # loop over all features\n",
    "            for feature in range(num_features):\n",
    "                # if feature is already in good features,\n",
    "                # skip this for loop\n",
    "                if feature in good_features:\n",
    "                    continue\n",
    "\n",
    "                # selected features are all good features till now\n",
    "                # and current feature\n",
    "                selected_features = good_features + [feature]\n",
    "\n",
    "                # remove all other features from data\n",
    "                xtrain = X[:, selected_features]\n",
    "\n",
    "                # calculate the score, in our case, AUC\n",
    "                score = self.evaluate_score(xtrain, y)\n",
    "\n",
    "                # if score is greater than the best score\n",
    "                # of this loop, change best score and best feature\n",
    "                if score > best_score:\n",
    "                    this_feature = feature\n",
    "                    best_score = score\n",
    "\n",
    "            # if we have selected a feature, add it\n",
    "            # to the good feature list and update best scores list\n",
    "            if this_feature != None:\n",
    "                good_features.append(this_feature)\n",
    "                best_scores.append(best_score)\n",
    "\n",
    "            # if we didnt improve during the last two rounds,\n",
    "            # exit the while loop\n",
    "            if len(best_scores) > 2:\n",
    "                if best_scores[-1] < best_scores[-2]:\n",
    "                        break\n",
    "\n",
    "        # return best scores and good features\n",
    "        # why do we remove the last data point?\n",
    "        return best_scores[:-1], good_features[:-1]\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # generate binary classification data\n",
    "    X, y = make_classification(n_samples=1000, n_features=100)\n",
    "    # transform data by greedy feature selection\n",
    "    X_transformed, scores = GreedyFeatureSelection()(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "\n",
    "In RFE, we start with all features and keep removing one feature in every iteration that\n",
    "provides the least value to a given model.\n",
    "\n",
    "To know what feature that gives the least value we can **analyse the coeficients** of our **SVM or \n",
    "logistic regression model.**\n",
    "\n",
    "In case of any **tree-based models**, we get feature\n",
    "importance in place of coefficients. In each iteration, we can eliminate the least important feature\n",
    "and keep eliminating it until we reach the number of features\n",
    "needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# fetch a regression dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "col_names = data[\"feature_names\"]\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"]\n",
    "\n",
    "# initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# initialize RFE\n",
    "rfe = RFE(estimator=model,\n",
    "          n_features_to_select=3)\n",
    "\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# get the transformed data with\n",
    "# selected columns\n",
    "X_transformed = rfe.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Coefficients or the Importance of Features\n",
    "\n",
    "Select features from the model. We can use a threshold on model coefficients to select features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fetch a regression dataset\n",
    "# in diabetes data we predict diabetes progression\n",
    "# after one year based on some features\n",
    "data = load_diabetes()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]\n",
    "\n",
    "# initialize the model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "idxs = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(idxs)), importances[idxs], align='center')\n",
    "plt.yticks(range(len(idxs)), [col_names[i] for i in idxs])\n",
    "plt.xlabel('Random Forest Feature Importance')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosse features using one model to use in another model\n",
    "\n",
    "For example, \n",
    "- use Logistic Regression coefficients to select the features \n",
    "- and then use Random Forest to train the model on chosen features. \n",
    "\n",
    "Scikit-learn also offers SelectFromModel class that helps you choose features directly from a given model. \n",
    "You can also specify the threshold for coefficients or feature importance if you want and the\n",
    "maximum number of features you want to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bmi', 's5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# fetch a regression dataset\n",
    "# in diabetes data we predict diabetes progression\n",
    "# after one year based on some features\n",
    "data = load_diabetes()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]\n",
    "\n",
    "# initialize the model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# select from the model\n",
    "sfm = SelectFromModel(estimator=model)\n",
    "X_transformed = sfm.fit_transform(X, y)\n",
    "\n",
    "# see which features were selected\n",
    "support = sfm.get_support()\n",
    "\n",
    "# get feature names\n",
    "print([x for x, y in zip(col_names, support) if y == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the most important features that were find in previous example. Thus, we could have also selected directly from feature importance provided by random forest.\n",
    "\n",
    "**Note:** All tree-based models provide feature importance and thus they can be used, for example:\n",
    "- XGBoost,\n",
    "- LightGBM \n",
    "- CatBoost. \n",
    "\n",
    "But using differnt models we can obtain different feature importance function.  \n",
    "In the end, we need to analyse if the features selected making sense for the problem that we are solving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Lasso penalization\n",
    "\n",
    "In L1 penalization for regularization, we can select the non-zero coefficients. \n",
    "Using the same code above is just replacing\n",
    "random forest in the snippet of selection from a model with a model that supports\n",
    "L1 penalty, e.g. lasso regression.\n",
    "\n",
    "**Final note:**\n",
    "Select features on training data and validate the model on validation data for proper selection of\n",
    "features without overfitting the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
